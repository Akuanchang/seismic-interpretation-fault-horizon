{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fault detection (inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's the whole cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "from copy import copy\n",
    "import h5py\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import scipy\n",
    "\n",
    "sys.path.append('../../..')\n",
    "\n",
    "from seismiqb import *\n",
    "\n",
    "from seismiqb.batchflow import FilesIndex, Pipeline\n",
    "from seismiqb.batchflow.research import Results\n",
    "from seismiqb.batchflow.models.torch import EncoderDecoder, ResBlock, TorchModel\n",
    "from seismiqb.batchflow import D, B, V, P, R, L, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './research_1d_without_transpose/results/crop_(1, 128, 192)-paths_01_ETP-repetition_0-update_0/2150108386/model_2000'\n",
    "inference_path = '/data/seismic_data/seismic_interpretation/CUBE_16_PSDM/PREDICTIONS/FAULTS/2150108386.hdf5'\n",
    "cube = '/data/seismic_data/seismic_interpretation/CUBE_16_PSDM/amplitudes_16_PSDM'\n",
    "cube_path = cube + '.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CROP_SHAPE = (1, 128, 192)\n",
    "BATCH_SIZE = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OVERLAP_SHAPE = tuple(np.maximum((np.array(CROP_SHAPE) * 3/4).astype(int), np.array([1,1,1])))\n",
    "STRIDE = (1, 64, 96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SeismicCubeset(FilesIndex(path=[cube_path], no_ext=True))\n",
    "dataset.load(label_dir={\n",
    "    'amplitudes_01_ETP': '/INPUTS/FAULTS/NPY/*',\n",
    "    'amplitudes_16_PSDM': '/INPUTS/FAULTS/NPY/*',\n",
    "}, labels_class=Fault, transform=True, verify=True)\n",
    "geometry = dataset.geometries[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on WHOLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry = dataset.geometries[0]\n",
    "path_hdf5 = inference_path\n",
    "\n",
    "# ! rm -r {path_hdf5}\n",
    "\n",
    "# file_hdf5 = h5py.File(path_hdf5, \"a\")\n",
    "# cube_hdf5 = file_hdf5.create_dataset('cube', geometry.cube_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference_template = (\n",
    "#     Pipeline()\n",
    "#     # Initialize everything\n",
    "#     .init_variable('result_preds', [])\n",
    "#     .load_model(mode='dynamic', model_class=TorchModel, name='model', path=model_path)\n",
    "#     # Load data\n",
    "#     .crop(points=D('grid_gen')(),\n",
    "#           shape=CROP_SHAPE)\n",
    "#     .load_cubes(dst='images')\n",
    "#     .adaptive_reshape(src='images', shape=CROP_SHAPE)\n",
    "#     .scale(mode='q', src='images')\n",
    "\n",
    "#     # Predict with model, then aggregate\n",
    "#     .predict_model('model',\n",
    "#                    B('images'),\n",
    "#                    fetches='sigmoid',\n",
    "#                    save_to=V('result_preds', mode='e'))\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_cube(geometry, crop_shape, stride, length):\n",
    "#     il_min = 0\n",
    "#     ranges = []\n",
    "#     for il_max in range(length, geometry.ilines_len + stride, stride):\n",
    "#         if il_max >= geometry.ilines_len:\n",
    "#             il_max = geometry.ilines_len - 1\n",
    "#         if il_max - il_min >= crop_shape[0]:\n",
    "#             ranges += [[il_min, il_max]]\n",
    "#         else:\n",
    "#             ranges[-1][-1] = geometry.ilines_len-1\n",
    "#         il_min += stride\n",
    "#     return ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for il_min, il_max in split_cube(geometry, CROP_SHAPE, 80, 140):\n",
    "#     print('bounds', il_min, il_max)\n",
    "#     dataset.make_grid(dataset.indices[0], CROP_SHAPE,\n",
    "#                       [il_min, il_max], [0, geometry.xlines_len-1], [0, geometry.depth-1],\n",
    "#                       strides=STRIDE,\n",
    "#                       batch_size=8) \n",
    "\n",
    "#     inference_pipeline = inference_template << dataset\n",
    "#     for _ in tqdm(range(dataset.grid_iters)):\n",
    "#         batch = inference_pipeline.next_batch(D('size'))\n",
    "\n",
    "#     # Write to hdf5\n",
    "#     slices = tuple([slice(*item) for item in dataset.grid_info['range']])\n",
    "#     prediction = (dataset.assemble_crops(inference_pipeline.v('result_preds'), order=(0, 1, 2)) > 0.5).astype(int)\n",
    "#     if il_min == 0:\n",
    "#         cube_hdf5[:100, slices[1], slices[2]] = prediction[0:100]\n",
    "#     elif il_max == geometry.ilines_len - 1:\n",
    "#         cube_hdf5[il_min+20:-1, slices[1], slices[2]] = prediction[20:]\n",
    "#     else:\n",
    "#         cube_hdf5[il_min+20:il_max-20, slices[1], slices[2]] = prediction[20:120]\n",
    "        \n",
    "    \n",
    "# file_hdf5.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry_sgy = SeismicGeometry(path_hdf5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit, prange\n",
    "\n",
    "@njit(parallel=True)\n",
    "def filter_array(array, result, window):\n",
    "    for i in prange(0, array.shape[0]-window[0]+1):\n",
    "        for j in prange(0, array.shape[1]-window[1]+1):\n",
    "            for k in prange(0, array.shape[2]-window[2]+1):\n",
    "                region = array[i:i+window[0], j:j+window[1], k:k+window[2]]\n",
    "                denum = np.sum(region**2) * region.shape[0] * region.shape[1]\n",
    "                if denum != 0:\n",
    "                    result[i, j, k] = ((np.sum(np.sum(region, axis=0), axis=0)**2).sum()) / denum\n",
    "    return result\n",
    "\n",
    "def semblance(geometry, labels=None, locations=None, window=10):\n",
    "    if isinstance(window, int):\n",
    "        window = np.ones(3, dtype=np.int32) * window\n",
    "    if labels:\n",
    "        _min = geometry.cube_shape\n",
    "        _max = np.zeros(3)\n",
    "        for label in labels:\n",
    "            _min = np.minimum(label.points.min(axis=0)+1, _min)\n",
    "            _max =  np.maximum(label.points.max(axis=0)+1, _max)\n",
    "    else:\n",
    "        if locations:\n",
    "            _min, _max = locations\n",
    "            _min = np.array(_min)\n",
    "            _max = np.array(_max)\n",
    "        else:\n",
    "            _min = np.zeros(3)\n",
    "            _max = geometry.cube_shape\n",
    "        \n",
    "    _min = _min.astype(int)\n",
    "    _max = _max.astype(int)\n",
    "\n",
    "    cube = geometry.file_hdf5['cube'][_min[0]:_max[0], _min[1]:_max[1], _min[2]:_max[2]]\n",
    "    window = np.minimum(np.array(window), cube.shape)\n",
    "\n",
    "    def compute_marfurt_semblance(region):\n",
    "        return (np.sum(region, axis=(0, 1))**2).sum() / (np.sum(region**2, axis=(0, 1)).sum() * len(region))\n",
    "\n",
    "    return scipy.ndimage.generic_filter(cube,\n",
    "                                        compute_marfurt_semblance,\n",
    "                                        window)\n",
    "    result = np.zeros_like(cube)\n",
    "    return filter_array(cube, result, window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = 743# dataset.labels[0][1].points[1,0]\n",
    "\n",
    "locations = geometry.make_slide_locations(loc, axis=0)\n",
    "shape = np.array([(slc.stop - slc.start) for slc in locations])\n",
    "seismic_slide = geometry.load_slide(loc)\n",
    "\n",
    "mask = np.zeros_like(seismic_slide.reshape(shape))\n",
    "for label in dataset.labels[0]:\n",
    "    mask = label.add_to_mask(mask, locations)\n",
    "    \n",
    "prediction = geometry_sgy.load_slide(loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sem = semblance(geometry, locations=[[locations[i].start for i in range(3)], [locations[i].stop for i in range(3)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(target, prediction):\n",
    "    target = 1 - (np.nan_to_num(target, 0))\n",
    "    return (prediction * target).sum() / prediction.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semblance = scipy.ndimage.generic_filter(seismic_slide,\n",
    "#                                          compute_marfurt_semblance,\n",
    "#                                          window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem.shape, prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou(sem, prediction), iou(sem, mask[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction[semblance > 0.9] = 0\n",
    "\n",
    "zoom_slice = (slice(0, 1), slice(None, None), slice(1000, 1500))\n",
    "plot_image((sem[zoom_slice]), figsize=(20, 20), cmap='gray', show=True, colorbar=True)\n",
    "plot_image((sem[zoom_slice], mask[zoom_slice], prediction[zoom_slice[1:]]), figsize=(20, 20), mode='overlap', color=('red', 'blue'), show=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for slide in range(800):\n",
    "#     mask = geometry_sgy.load_slide(slide)\n",
    "#     if mask.sum() > 0:\n",
    "#         print(slide, mask.sum())\n",
    "\n",
    "slide = loc# dataset.labels[0][1].points[0, 0]\n",
    "\n",
    "zoom_slice = (slice(None, None), slice(1000, 1500))\n",
    "\n",
    "\n",
    "dataset.show_slide(slide, figsize=(20, 20), width=1, zoom_slice=zoom_slice)\n",
    "\n",
    "mask = geometry_sgy.load_slide(slide)\n",
    "slide = dataset.geometries[0].load_slide(slide)\n",
    "plot_image((slide[:, zoom_slice[1]], mask[:, zoom_slice[1]]), mode='overlap', figsize=(20, 20), zoom_slice=zoom_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
