{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fault detection (Research)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will demonstrate the research process of fault detection models. We will train multiple 2D and 3D models on all posible combinations of seismic cubes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from copy import copy\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "\n",
    "sys.path.append('../../..')\n",
    "\n",
    "from seismiqb import *\n",
    "\n",
    "from seismiqb.batchflow import FilesIndex, Pipeline\n",
    "from seismiqb.batchflow.research import Option, Research, RP, RC, RD, REP, KV, RI\n",
    "from seismiqb.batchflow.models.torch import EncoderDecoder, ResBlock\n",
    "from seismiqb.batchflow import D, B, V, P, R, L, W, C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we describe model configuration. In general it is UNet-like architecture where we fix some parameters but all of them are also is a subject of research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Dice(nn.Module):\n",
    "    def forward(self, input, target):\n",
    "        input = torch.sigmoid(input)\n",
    "        dice_coeff = 2. * (input * target).sum() / (input.sum() + target.sum() + 1e-7)\n",
    "        return 1 - dice_coeff\n",
    "\n",
    "ITERS = 2000\n",
    "BATCH_SIZE = 96\n",
    "FILTERS = [64, 96, 128, 192, 256]\n",
    "\n",
    "MODEL_CONFIG = {\n",
    "    # Model layout\n",
    "    'initial_block': {\n",
    "        'base_block': ResBlock,\n",
    "        'filters': FILTERS[0] // 2,\n",
    "        'kernel_size': 5,\n",
    "        'downsample': False,\n",
    "        'attention': 'scse'\n",
    "    },\n",
    "\n",
    "    'body/encoder': {\n",
    "        'num_stages': 4,\n",
    "        'order': 'sbd',\n",
    "        'blocks': {\n",
    "            'base': ResBlock,\n",
    "            'n_reps': 1,\n",
    "            'filters': FILTERS[:-1],\n",
    "            'attention': 'scse',\n",
    "        },\n",
    "    },\n",
    "    'body/embedding': {\n",
    "        'base': ResBlock,\n",
    "        'n_reps': 1,\n",
    "        'filters': FILTERS[-1],\n",
    "        'attention': 'scse',\n",
    "    },\n",
    "    'body/decoder': {\n",
    "        'num_stages': 4,\n",
    "        'upsample': {\n",
    "            'layout': 'tna',\n",
    "            'kernel_size': 2,\n",
    "        },\n",
    "        'blocks': {\n",
    "            'base': ResBlock,\n",
    "            'filters': FILTERS[-2::-1],\n",
    "            'attention': 'scse',\n",
    "        },\n",
    "    },\n",
    "    'head': {\n",
    "        'base_block': ResBlock,\n",
    "        'filters': [16, 8],\n",
    "        'attention': 'scse'\n",
    "    },\n",
    "    'output': torch.sigmoid,\n",
    "    # Train configuration\n",
    "    'loss': Dice(),\n",
    "    'optimizer': {'name': 'Adam', 'lr': 0.005,},\n",
    "    \"decay\": {'name': 'exp', 'gamma': 0.1, 'frequency': 150},\n",
    "    'microbatch': 8,\n",
    "    'common/activation': 'relu6',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole training process is described by the following pipeline. We will vary crop shape (2D (1, 128, 256) and 3D (32, 128, 256)) so define it as a `C('crop')` to use with `Research` from `batchflow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pipeline = (\n",
    "    Pipeline()\n",
    "    # Initialize pipeline variables and model\n",
    "    .init_variable('loss_history', [])\n",
    "    .init_model('dynamic', EncoderDecoder, 'model', MODEL_CONFIG)\n",
    "\n",
    "    # Load data/masks\n",
    "    .crop(points=D('train_sampler')(BATCH_SIZE), shape=C('crop'), side_view=False)\n",
    "    .create_masks(dst='masks', width=1)\n",
    "    .mask_rebatch(src='masks', threshold=0.5, axis=(0, 1))\n",
    "    .load_cubes(dst='images')\n",
    "    .adaptive_reshape(src=['images', 'masks'], shape=C('crop'))\n",
    "    .scale(mode='q', src='images')\n",
    "\n",
    "    # Augmentations\n",
    "    .transpose(src=['images', 'masks'], order=(1, 2, 0))\n",
    "    .flip(axis=1, src=['images', 'masks'], seed=P(R('uniform', 0, 1)), p=0.3)\n",
    "    .additive_noise(scale=0.005, src='images', dst='images', p=0.3)\n",
    "    .rotate(angle=P(R('uniform', -15, 15)),\n",
    "            src=['images', 'masks'], p=0.3)\n",
    "    .scale_2d(scale=P(R('uniform', 0.85, 1.15)),\n",
    "              src=['images', 'masks'], p=0.3)\n",
    "    .transpose(src=['images', 'masks'], order=(2, 0, 1))\n",
    "\n",
    "    # Training\n",
    "    .train_model('model',\n",
    "                 fetches='loss',\n",
    "                 images=B('images'),\n",
    "                 masks=B('masks'),\n",
    "                 save_to=V('loss_history', mode='w'))\n",
    "    .run_later(D('size'), n_iters=ITERS)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we describe two callables: to create dataset from paths to cubes and to dump the resulting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(pipeline, config):\n",
    "    paths = list(config.config()['paths'])\n",
    "    dataset = SeismicCubeset(FilesIndex(path=paths, no_ext=True))\n",
    "    dataset.load(label_dir={\n",
    "        'amplitudes_01_ETP': '/INPUTS/FAULTS/NPY/*',\n",
    "        'amplitudes_16_PSDM': '/INPUTS/FAULTS/NPY/*',\n",
    "    }, labels_class=Fault, transform=True, verify=True)\n",
    "    dataset.modify_sampler(dst='train_sampler', finish=True, low=0.0, high=1.0)\n",
    "    pipeline.set_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_model(pipeline, path, iteration):\n",
    "    path = os.path.join(path, 'model_'+str(iteration))\n",
    "    pipeline.save_model_now('model', path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we construct all possible combinations of 4 availiable cubes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['/data/seismic_data/seismic_interpretation/CUBE_01_ETP/amplitudes_01_ETP.hdf5',\n",
    "            '/data/seismic_data/seismic_interpretation/CUBE_16_PSDM/amplitudes_16_PSDM.hdf5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SeismicCubeset(FilesIndex(path=datasets[1:], no_ext=True))\n",
    "\n",
    "dataset.load(label_dir={\n",
    "    'amplitudes_01_ETP': '/INPUTS/FAULTS/NPY/*',\n",
    "    'amplitudes_16_PSDM': '/INPUTS/FAULTS/NPY/*',\n",
    "}, labels_class=Fault, transform=True, verify=True)\n",
    "dataset.modify_sampler(dst='train_sampler', finish=True, low=0.0, high=1.0)\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    bounds = min([fault.points[:, 2].min() for fault in dataset.labels[i]]), max([fault.points[:, 2].max() for fault in dataset.labels[i]])\n",
    "    points = np.random.choice(len(dataset.labels[i]), 3, replace=False)\n",
    "    for p in points:\n",
    "        dataset.show_slide(dataset.labels[i][p].points[0, 0], idx=i,\n",
    "                           figsize=(10,10), zoom_slice = (slice(None, None), slice(*bounds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [list(itertools.combinations(datasets, i+1)) for i in range(2)]\n",
    "datasets = sum(datasets, [])\n",
    "datasets = [KV(item, '_'.join([os.path.splitext(os.path.basename(name))[0][11:] for name in item])) for item in datasets]\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, define domain of parameters: paths to cubes and crop_shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = Option('paths', datasets) * Option('crop', [\n",
    "    (1, 64, 128), (1, 64, 192), (1, 64, 256), (1, 64, 512),\n",
    "    (1, 128, 128), (1, 128, 192), (1, 128, 256), (1, 128, 512),\n",
    "    (1, 256, 256)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe experiment plan as a research-pipeline. We well use 8 availiable gpus to train models in parallel to increase the speed several times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! rm -r research_1d_without_transpose\n",
    "\n",
    "research = (Research()\n",
    "    .init_domain(domain)\n",
    "    .add_callable(create_dataset, pipeline=RP('train'), config=RC(), execute='#0')\n",
    "    .add_pipeline(train_pipeline, name='train', variables='loss_history', logging=True)\n",
    "    .add_callable(dump_model, pipeline=RP('train'), path=REP(), iteration=RI(), execute=[100, 'last'], logging=True)\n",
    ")\n",
    "\n",
    "research.run(name='research_1d_without_transpose', workers=6, devices=[0, 1, 2, 3, 4, 5], timeout=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot loss functions of all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = research.load_results().df\n",
    "(results\n",
    " .pivot(index='iteration', columns='sample_index', values='loss_history')\n",
    " .rolling(100).mean()\n",
    " .plot(xlim=(100, 2000), legend=True, title='loss')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = results.groupby('sample_index').apply(lambda x: x.iloc[-1]['loss_history']).idxmin()\n",
    "results[results.sample_index == sample_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
