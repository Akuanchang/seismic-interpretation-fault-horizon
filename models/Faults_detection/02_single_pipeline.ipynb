{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fault detection (Research)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will demonstrate the research process of fault detection models. We will train multiple 2D and 3D models on all posible combinations of seismic cubes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from copy import copy\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "\n",
    "sys.path.append('../../..')\n",
    "\n",
    "from seismiqb import *\n",
    "\n",
    "from seismiqb.batchflow import FilesIndex, Pipeline\n",
    "from seismiqb.batchflow.research import Option, Research, RP, RC, RD, REP, KV, RI\n",
    "from seismiqb.batchflow.models.torch import EncoderDecoder, ResBlock\n",
    "from seismiqb.batchflow import D, B, V, P, R, L, W, C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we describe model configuration. In general it is UNet-like architecture where we fix some parameters but all of them are also is a subject of research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Dice(nn.Module):\n",
    "    def forward(self, input, target):\n",
    "        input = torch.sigmoid(input)\n",
    "        dice_coeff = 2. * (input * target).sum() / (input.sum() + target.sum() + 1e-7)\n",
    "        return 1 - dice_coeff\n",
    "\n",
    "ITERS = 2000\n",
    "BATCH_SIZE = 96\n",
    "FILTERS = [64, 96, 128, 192, 256]\n",
    "\n",
    "MODEL_CONFIG = {\n",
    "    # Model layout\n",
    "    'initial_block': {\n",
    "        'base_block': ResBlock,\n",
    "        'filters': FILTERS[0] // 2,\n",
    "        'kernel_size': 5,\n",
    "        'downsample': False,\n",
    "        'attention': 'scse'\n",
    "    },\n",
    "\n",
    "    'body/encoder': {\n",
    "        'num_stages': 4,\n",
    "        'order': 'sbd',\n",
    "        'blocks': {\n",
    "            'base': ResBlock,\n",
    "            'n_reps': 1,\n",
    "            'filters': FILTERS[:-1],\n",
    "            'attention': 'scse',\n",
    "        },\n",
    "    },\n",
    "    'body/embedding': {\n",
    "        'base': ResBlock,\n",
    "        'n_reps': 1,\n",
    "        'filters': FILTERS[-1],\n",
    "        'attention': 'scse',\n",
    "    },\n",
    "    'body/decoder': {\n",
    "        'num_stages': 4,\n",
    "        'upsample': {\n",
    "            'layout': 'tna',\n",
    "            'kernel_size': 2,\n",
    "        },\n",
    "        'blocks': {\n",
    "            'base': ResBlock,\n",
    "            'filters': FILTERS[-2::-1],\n",
    "            'attention': 'scse',\n",
    "        },\n",
    "    },\n",
    "    'head': {\n",
    "        'base_block': ResBlock,\n",
    "        'filters': [16, 8],\n",
    "        'attention': 'scse'\n",
    "    },\n",
    "    'output': torch.sigmoid,\n",
    "    # Train configuration\n",
    "    'loss': Dice(),\n",
    "    'optimizer': {'name': 'Adam', 'lr': 0.005,},\n",
    "    \"decay\": {'name': 'exp', 'gamma': 0.1, 'frequency': 150},\n",
    "    'microbatch': 8,\n",
    "    'common/activation': 'relu6',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole training process is described by the following pipeline. We will vary crop shape (2D (1, 128, 256) and 3D (32, 128, 256)) so define it as a `C('crop')` to use with `Research` from `batchflow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/data/seismic_data/seismic_interpretation/CUBE_16_PSDM/amplitudes_16_PSDM.hdf5'\n",
    "LABELS_PATH = '/data/seismic_data/seismic_interpretation/CUBE_16_PSDM/INPUTS/FAULTS/HDF5/faults.hdf5'\n",
    "dataset = SeismicCubeset(FilesIndex(path=PATH, no_ext=True))\n",
    "\n",
    "dataset.load(label_dir={\n",
    "    'amplitudes_01_ETP': '/INPUTS/FAULTS/NPY/*',\n",
    "    'amplitudes_16_PSDM': '/INPUTS/FAULTS/NPY/*',\n",
    "}, labels_class=Fault, transform=True, verify=True)\n",
    "\n",
    "dataset.modify_sampler(dst='train_sampler', finish=True, low=0.0, high=1.0)\n",
    "dataset.labels['amplitudes_16_PSDM'] = SeismicGeometry(LABELS_PATH, geometry=dataset.geometries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pipeline = (\n",
    "    Pipeline()\n",
    "    # Initialize pipeline variables and model\n",
    "    .init_variable('loss_history', [])\n",
    "    .init_model('dynamic', EncoderDecoder, 'model', MODEL_CONFIG)\n",
    "    # Load data/masks\n",
    "    .crop(points=D('train_sampler')(BATCH_SIZE), shape=C('crop'), side_view=False)\n",
    "    #.create_masks(dst='masks', width=1)\n",
    "    #.mask_rebatch(src='masks', threshold=0.5, axis=(0, 1))\n",
    "    .load_cubes(dst='images')\n",
    "    .load_cubes(dst='masks', src_geometry='labels')\n",
    "    .adaptive_reshape(src=['images', 'masks'], shape=C('crop'))\n",
    "#     .scale(mode='q', src='images')\n",
    "\n",
    "#     # Augmentations\n",
    "#     .transpose(src=['images', 'masks'], order=(1, 2, 0))\n",
    "#     .flip(axis=1, src=['images', 'masks'], seed=P(R('uniform', 0, 1)), p=0.3)\n",
    "#     .additive_noise(scale=0.005, src='images', dst='images', p=0.3)\n",
    "#     .rotate(angle=P(R('uniform', -15, 15)),\n",
    "#             src=['images', 'masks'], p=0.3)\n",
    "#     .scale_2d(scale=P(R('uniform', 0.85, 1.15)),\n",
    "#               src=['images', 'masks'], p=0.3)\n",
    "#     .transpose(src=['images', 'masks'], order=(2, 0, 1))\n",
    "\n",
    "    # Training\n",
    "    .train_model('model',\n",
    "                 fetches='loss',\n",
    "                 images=B('images'),\n",
    "                 masks=B('masks'),\n",
    "                 save_to=V('loss_history', mode='w'))\n",
    "    .run_later(D('size'), n_iters=10, profile=True, bar=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pipeline.set_config({'crop': (1, 128, 128)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl = train_pipeline << dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = ppl.next_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.plot_components('images', 'masks',\n",
    "                  slide=0, mode='overlap',\n",
    "                  idx=9, alpha=[0.9, 0.5],\n",
    "                  title='_e_psdm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl.show_profile_info(per_iter=False, detailed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
