{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative research\n",
    "\n",
    "In this advanced notebook, we apply the [carcass interpolation model](./01_Demo_E.ipynb), as well as [horizon extension](./../Horizon_extension/Demo_E.ipynb) and enhancement ones in a quick succesion with the help of [research](./../Research_template.ipynb). It is adviced to check out our notebooks on these techniques prior to looking at this one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "from glob import glob\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import date\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('../../..')\n",
    "from seismiqb.batchflow import Pipeline, Dataset\n",
    "from seismiqb.batchflow.models.torch import ResBlock\n",
    "from seismiqb.batchflow.research import Research, Option, Domain, FileLogger\n",
    "from seismiqb.batchflow.research import RP, RC, KV\n",
    "\n",
    "from seismiqb import SeismicGeometry, Horizon, HorizonMetrics, plot_image\n",
    "\n",
    "from seismiqb.src.controllers.torch_models import EncoderDecoder, ExtensionModel\n",
    "from seismiqb import MODEL_CONFIG_DETECTION, MODEL_CONFIG_EXTENSION\n",
    "from seismiqb import BaseController, Interpolator, Enhancer, Extender\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_metric_in_holes(horizon_with_holes, true_horizon, predicted_horizon):\n",
    "    metric = np.where((true_horizon.full_matrix != true_horizon.FILL_VALUE) & \\\n",
    "                      (predicted_horizon.full_matrix != predicted_horizon.FILL_VALUE) & \\\n",
    "                      (horizon_with_holes.full_matrix == horizon_with_holes.FILL_VALUE),\n",
    "                       np.abs(true_horizon.full_matrix - predicted_horizon.full_matrix), np.nan)\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global parameters\n",
    "DEVICES = [1, 3, 4]         # physical device numbers\n",
    "WORKERS = len(DEVICES)\n",
    "\n",
    "RESEARCH_NAME = f'Research_horizons'\n",
    "\n",
    "DUMP_NAME = date.today().strftime(\"%Y-%m-%d\") + RESEARCH_NAME[8:]\n",
    "N_REPS = 1\n",
    "\n",
    "FREQUENCIES = (200, 200)\n",
    "ITERATIONS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_CONFIG = {\n",
    "    'savedir': None,\n",
    "    'logger': None,\n",
    "    'monitor': False,\n",
    "    'bar': False,\n",
    "    'plot': False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTENSION_CONFIG = {\n",
    "    'savedir': None,\n",
    "    'monitor': True,\n",
    "    'bar': False,\n",
    "    'plot': False,\n",
    "    'sampler': {},\n",
    "    'train': {\n",
    "        'model_class': ExtensionModel,\n",
    "        'model_config': {\n",
    "            **MODEL_CONFIG_EXTENSION,\n",
    "            'microbatch': 16,\n",
    "        },\n",
    "        \n",
    "        'batch_size': 64,\n",
    "        'crop_shape': (1, 128, 128),\n",
    "\n",
    "        'adaptive_slices': False,\n",
    "        'side_view': True,\n",
    "        'width': 5,\n",
    "\n",
    "        'rebatch_threshold': 0.7,\n",
    "        'rescale_batch_size': True,\n",
    "        \n",
    "        'prefetch': 3,\n",
    "        'n_iters': 300,\n",
    "        'early_stopping': True,\n",
    "    },\n",
    "    'inference': {\n",
    "        'batch_size': 128,\n",
    "        'crop_shape': (1, 128, 128),\n",
    "        'prefetch': 0,\n",
    "        \n",
    "        'width': 5,\n",
    "        \n",
    "        'n_steps': 50,\n",
    "        'stride': 32,\n",
    "    },\n",
    "    'evaluate': {\n",
    "        'n': 1,\n",
    "        'supports': 100,\n",
    "        'dump': True,\n",
    "        'device': 'gpu',\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\n",
    "    ('/data/seismic_data/seismic_interpretation/CUBE_01_ETP/amplitudes_01_ETP.hdf5',\n",
    "     '/data/seismic_data/seismic_interpretation/CUBE_01_ETP/INPUTS/HORIZONS/RAW/etp*'),\n",
    "    ('/data/seismic_data/seismic_interpretation/CUBE_18_UNKNOWN/amplitudes_18_UNKNOWN.hdf5',\n",
    "     '/data/seismic_data/seismic_interpretation/CUBE_18_UNKNOWN/INPUTS/HORIZONS/RAW/*'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrolled = [\n",
    "    (cube_path, horizon_path)\n",
    "    for cube_path, horizon_dir in paths\n",
    "    for horizon_path in glob(horizon_dir)\n",
    "]\n",
    "\n",
    "options = [\n",
    "    KV((cube_path, horizon_path),\n",
    "       '+'.join((cube_path.split('/')[-1].split('.')[0], horizon_path.split('/')[-1].split('.')[0])))\n",
    "    for cube_path, horizon_path in unrolled\n",
    "]\n",
    "random.shuffle(options)\n",
    "\n",
    "domain = (\n",
    "    Option('cube_and_horizon', options) * \n",
    "    Option('seed', [0, 1, 2])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_one_experiment(config):\n",
    "\n",
    "    ###################################################################################\n",
    "    ################################   PARSE CONFIGS   ################################\n",
    "    ###################################################################################\n",
    "    # Get all the params from configs\n",
    "    config = config.config()\n",
    "    cube_path, horizon_path = config['cube_and_horizon']\n",
    "    seed = config['seed']\n",
    "    n_rep = config['repetition']\n",
    "    \n",
    "    # Directory to save results to\n",
    "    results_dir = os.path.join(RESEARCH_NAME, 'custom_results')\n",
    "    \n",
    "    short_name_cube = cube_path.split('/')[-1].split('.')[0]\n",
    "    short_name_horizon = horizon_path.split('/')[-1].split('.')[0]\n",
    "    alias = os.path.join(short_name_cube, short_name_horizon, f'{n_rep}')\n",
    "    savedir = os.path.join(results_dir, alias)\n",
    "    \n",
    "    return_value = [[], [], [], []]   # coverages, window ratios, support corrs, phases\n",
    "    \n",
    "\n",
    "    ###################################################################################\n",
    "    ####################################    BASE    ###################################\n",
    "    ###################################################################################\n",
    "    base_config = {\n",
    "        **BASE_CONFIG,\n",
    "        'savedir': savedir,\n",
    "    }\n",
    "    controller = BaseController(base_config)\n",
    "    controller.log(f'Seed at this exp: {seed}')\n",
    "    \n",
    "\n",
    "    ###################################################################################\n",
    "    #############################   HORIZON PREPARATION   #############################\n",
    "    ###################################################################################\n",
    "    interpolator = Interpolator()\n",
    "    train_dataset = interpolator.make_dataset(cube_paths=cube_path, horizon_paths=horizon_path)\n",
    "    horizon = train_dataset.labels[0][0].__copy__()\n",
    "    filtering_matrix = horizon.make_random_holes_matrix(n=20, points_proportion=1e-5, points_shape=5, \n",
    "                                                        noise_level=25, seed=seed)\n",
    "    horizon.filter(filtering_matrix=filtering_matrix)\n",
    "    horizon_with_holes = horizon.__copy__()\n",
    "    \n",
    "    \n",
    "    ###################################################################################\n",
    "    ##################################   EXTENSION   ##################################\n",
    "    ###################################################################################\n",
    "    for i in range(ITERATIONS):\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        extension_config = {\n",
    "            **EXTENSION_CONFIG,\n",
    "            'savedir': f'{savedir}/{1+i}_extension',\n",
    "            'logger': controller.filelogger,\n",
    "        }\n",
    "        extender = Extender(extension_config)\n",
    "        \n",
    "        model = extender.train(horizon=horizon)\n",
    "        horizon = extender.inference(horizon, model)\n",
    "        horizon = extender.postprocess(horizon)\n",
    "        \n",
    "        info = extender.evaluate(horizon, dataset=train_dataset)[0]\n",
    "        \n",
    "        return_value[0].append(horizon.coverage)\n",
    "        return_value[1].append(info['window_rate'])\n",
    "        return_value[2].append(info['corrs'])\n",
    "        return_value[3].append(info['phase'])\n",
    "        \n",
    "        l1_metric_matrix = l1_metric_in_holes(horizon_with_holes, train_dataset.labels[0][0], horizon)\n",
    "        l1 = np.nanmean(l1_metric_matrix)\n",
    "        controller.log(f'l1 in holes at this exp: {l1}')\n",
    "        plot_image(l1_metric_matrix, \n",
    "                   plot=True, show=extender.plot,\n",
    "                   savepath=extender.make_savepath('my_custom_l1.png'))\n",
    "\n",
    "\n",
    "    ###################################################################################\n",
    "    ##############################   SAVE NEXT TO CUBE   ##############################\n",
    "    ###################################################################################\n",
    "    cube_dir = os.path.dirname(horizon.geometry.path)\n",
    "    savepath = os.path.join(cube_dir, 'PREDICTIONS/HORIZONS', DUMP_NAME)\n",
    "    os.makedirs(savepath, exist_ok=True)\n",
    "\n",
    "    horizon.name = '+' + horizon.name.replace('enhanced_', '').replace('extended_', '')\n",
    "    if N_REPS != 1:\n",
    "        horizon.name += f'_{n_rep}'\n",
    "\n",
    "    savepath = os.path.join(savepath, horizon.name)\n",
    "    horizon.dump_float(savepath, add_height=False)\n",
    "    interpolator.log(f'Dumped horizon to {savepath}')\n",
    "\n",
    "\n",
    "    ###################################################################################\n",
    "    ###################################   RETURNS   ###################################\n",
    "    ###################################################################################\n",
    "    \n",
    "    msg = f'Finished experiment:\\n{\"\"*60}{horizon.name}\\n'\n",
    "    for name, value in zip(returned_values, return_value):\n",
    "        msg += f'{\"\"*60}{name} -> {value}\\n'\n",
    "    interpolator.log(msg)\n",
    "    return return_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research Research_horizons is starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Domain updated: 0: 100%|██████████| 3/3.0 [07:51<00:00, 157.23s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<seismiqb.batchflow.research.research.Research at 0x7f5174011390>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!rm -rf {RESEARCH_NAME}\n",
    "\n",
    "returned_values = [\n",
    "    'coverages', 'window_rates', 'corrs', 'phases',\n",
    "]\n",
    "\n",
    "# Fake pipeline is needed to pass parameters around\n",
    "fake_ppl = Pipeline().set_dataset(Dataset(10)).run_later(1, n_iters=1)\n",
    "\n",
    "research = (\n",
    "    Research()\n",
    "    .add_logger(FileLogger)\n",
    "    .init_domain(domain, n_reps=N_REPS)\n",
    "    .add_pipeline(fake_ppl, run=True, name='fake')\n",
    "    .add_callable(\n",
    "        perform_one_experiment,                         # Callable to run\n",
    "        returns=returned_values,                        # Names of returned results\n",
    "        execute='#0',                                   # Execute immediately\n",
    "        config=RC('fake'),                              # Pass config to the callable\n",
    "        name='perform_one_experiment'                   # Name to be shown in the dataframe\n",
    "    )\n",
    ")\n",
    "\n",
    "research.run(\n",
    "    n_iters=1,\n",
    "    name=RESEARCH_NAME,\n",
    "    bar=True,\n",
    "    workers=WORKERS,\n",
    "    devices=DEVICES,\n",
    "    timeout=10000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average horizons\n",
    "If each carcass is interpolated multiple times, we can aggregate repetitions into an averaged surface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if N_REPS > 1:\n",
    "\n",
    "    for cube_path, _ in paths:\n",
    "        # Parse paths and make directory for saving averaged horizons\n",
    "        cube_dir = os.path.dirname(cube_path)\n",
    "        horizon_dir = os.path.join(cube_dir, 'PREDICTIONS/HORIZONS', DUMP_NAME)\n",
    "        savedir = horizon_dir + '_AVERAGED'\n",
    "        os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "        # Load all the predictions for a given cube\n",
    "        geometry = SeismicGeometry(cube_path)\n",
    "        horizons = [Horizon(path, geometry) for path in glob(horizon_dir + '/*')]\n",
    "        names = set(['_'.join(horizon.name.split('_')[:-1]) for horizon in horizons])\n",
    "\n",
    "        # Average\n",
    "        for name in sorted(names):\n",
    "            current_horizons = [horizon for horizon in horizons\n",
    "                                if horizon.name.startswith(name)]\n",
    "\n",
    "            averaged, dct = Horizon.average_horizons(current_horizons)\n",
    "            plot_image(dct['std_matrix'], title=f'Averaged {name}')\n",
    "            plt.show()\n",
    "\n",
    "            savepath = os.path.join(savedir, name)\n",
    "            averaged.dump_float(savepath)\n",
    "            print('Dumped to', savepath)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select best horizon\n",
    "If there are multiple repetitions, we can select the best one, based on metrics (support correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if N_REPS > 1:\n",
    "\n",
    "    for cube_path, _ in paths:\n",
    "        # Parse paths and make directory for saving averaged horizons\n",
    "        cube_dir = os.path.dirname(cube_path)\n",
    "        horizon_dir = os.path.join(cube_dir, 'PREDICTIONS/HORIZONS', DUMP_NAME)\n",
    "        averaged_dir = horizon_dir + '_AVERAGED'\n",
    "        \n",
    "        savedir = horizon_dir + '_BEST'\n",
    "        os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "        # Load all the predictions for a given cube, as well as averaged versions of them\n",
    "        geometry = SeismicGeometry(cube_path)\n",
    "        horizons = [Horizon(path, geometry) for path in glob(horizon_dir + '/*')]\n",
    "        averaged = [Horizon(path, geometry) for path in glob(averaged_dir + '/*')]\n",
    "        names = set(['_'.join(horizon.name.split('_')[:-1]) for horizon in horizons])\n",
    "\n",
    "        # Select the best one\n",
    "        for name in sorted(names):\n",
    "            current_horizons = [horizon for horizon in horizons\n",
    "                                if horizon.name.startswith(name)]\n",
    "            current_horizons += [horizon for horizon in averaged\n",
    "                                 if name in horizon.name]\n",
    "            \n",
    "            values = []\n",
    "            for horizon in current_horizons:\n",
    "                hm = HorizonMetrics(horizon)\n",
    "                correlation_map = hm.evaluate('support_corrs', supports=100, device='gpu', plot=False, show=False)\n",
    "                values.append(np.nanmean(correlation_map))\n",
    "            idx = np.argmax(values)\n",
    "            best = current_horizons[idx]\n",
    "\n",
    "            savepath = os.path.join(savedir, name)\n",
    "            best.dump_float(savepath)\n",
    "            print(f'Dumped {idx} to {savepath}')\n",
    "            print(f'MeanMetrics are {[round(item, 3) for item in values]}')\n",
    "            print(f'Coverages   are {[round(horizon.coverage, 3) for horizon in current_horizons]}\\n')\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
